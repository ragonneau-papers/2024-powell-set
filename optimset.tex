%% optimset.tex
%% Copyright 2023 Tom M. Ragonneau and Zaikun Zhang
\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[a4paper]{geometry}
\usepackage[british]{babel}
\usepackage{microtype}

% Font and encoding
\usepackage{lmodern}
\usepackage[T1]{fontenc}

% Line spacing
\usepackage[nodisplayskipstretch]{setspace}
\onehalfspacing

% Font Awesome 5
\usepackage{fontawesome5}
\usepackage[dvipsnames]{xcolor}
\definecolor{ORCIDGreen}{HTML}{A6CE39}
\definecolor{LinkedInBlue}{HTML}{0077B5}
\definecolor{TwitterBlue}{HTML}{1DA1F2}

% Cross-referencing
\usepackage{textgreek}
\usepackage[
    pdfusetitle,
    hyperfootnotes=false,
]{hyperref}
\usepackage{url}
\hypersetup{
    final,
    colorlinks=true,
    linktoc=page,
    linkcolor=OliveGreen,
    anchorcolor=black,
    citecolor=MidnightBlue,
    filecolor=black,
    menucolor=black,
    runcolor=black,
    urlcolor=black,
}
\newcommand*{\email}[2][inbox]{Email: \href{mailto:#2}{#2}}
\newcommand*{\orcid}[1]{ORCID: \href{https://orcid.org/#1}{#1}}

% Mathematics
\usepackage{mathtools}
\usepackage{cases}
\usepackage[bb=dsfontserif]{mathalfa}
\numberwithin{equation}{section}

% Line numbering in draft mode
\usepackage{lineno}
\usepackage{ifdraft}
\newcommand{\marginfont}{\scriptsize\sffamily\color{gray}}
\renewcommand{\linenumberfont}{\normalfont\marginfont}
\ifdraft{\linenumbers}{}

% Show labels in draft mode
\ifdraft{}{\PassOptionsToPackage{final}{showlabels}}
\usepackage{showlabels}
\renewcommand{\showlabelfont}{\linespread{1}\marginfont}

% To-do notes in draft mode
\usepackage[
    marginface=\linespread{1}\marginfont,
    author={},
]{fixme}

% Modify the typesets of the `\maketitle' and `\thanks' commands
\usepackage{titling}
\setlength{\thanksmargin}{0pt}

% Mathematical environments (theorems, proofs, ...)
\usepackage{amsthm}
\theoremstyle{definition}
\newtheorem{assumption}{Assumption}[section]
\newtheorem{axiom}{Axiom}[section]
\newtheorem{condition}{Condition}[section]
\newtheorem{definition}{Definition}[section]
\newtheorem{example}{Example}[section]
\newtheorem{hypothesis}{Hypothesis}[section]
\newtheorem{problem}{Problem}[section]
\newtheorem{property}{Property}[section]
\newtheorem{question}{Question}[section]
\theoremstyle{plain}
\newtheorem{assertion}{Assertion}[section]
\newtheorem{proposition}{Proposition}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{theorem}{Theorem}[section]
\newtheorem{conjecture}{Conjecture}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{criterion}{Criterion}[section]
\theoremstyle{remark}
\newtheorem*{acknowledgment}{Acknowledgment}
\newtheorem*{annotation}{Annotation}
\newtheorem*{case}{case}
\newtheorem*{claim}{Claim}
\newtheorem*{conclusion}{Conclusion}
\newtheorem*{notation}{Notation}
\newtheorem*{note}{Note}
\newtheorem*{remark}{Remark}
\newtheorem*{summary}{Summary}

% `\keywords' and `\msc' commands
\newcommand{\createlist}[3]{
    \expandafter\newcommand\expandafter{\csname #1\endcsname}[1]{
        \begingroup
        \def\and{ ; }
        \hypersetup{#2={##1}}
        \def\and{\ifhmode\unskip\nobreak\fi\ $\cdot$ }
        \paragraph*{#3} ##1
        \endgroup
    }
}
\createlist{keywords}{pdfkeywords}{Keywords}
\createlist{msc}{pdfsubject}{Mathematics Subject Classification (2010)}

% Checks page or column breaks for issues with widow or orphan lines
\usepackage[prevent-all]{widows-and-orphans}

% no line break in inline math
\interdisplaylinepenalty=10000
\relpenalty=10000
\binoppenalty=10000

% Prevent footnotes from running to the next page
\interfootnotelinepenalty=10000

% Make @ behave as per catcode 13 (active). TeXbook p. 155.
\mathcode`@="8000{\catcode`\@=\active\gdef@{\mkern1mu}}

% Glossaries
\usepackage[acronym]{glossaries}
\glsdisablehyper
\newacronym{dfo}{DFO}{derivative-free optimization}

% Control sequences, macros, and definitions
\usepackage{etoolbox}
\usepackage{xspace}
\usepackage{bbm}
\DeclareMathOperator{\bigo}{\mathcal{O}}
\DeclareMathOperator{\card}{card}
\newcommand*{\abs}[2][]{#1\lvert#2#1\rvert}
\newcommand*{\fset}{\Omega}
\newcommand*{\lagp}[1][]{L\ifblank{#1}{}{_{#1}}}
\newcommand*{\norm}[2][]{#1\lVert#2#1\rVert}
\newcommand*{\obj}{f}
\newcommand*{\objm}[1][]{\hat{\obj}\ifblank{#1}{}{^{#1}}}
\newcommand*{\qpoly}{\mathcal{Q}_n}
\newcommand*{\R}{\mathbb{R}}
\newcommand*{\set}[2][]{#1\{#2#1\}}
\newcommand*{\solvername}[1]{\textsc{#1}\xspace}
\newcommand*{\T}{\mathsf{T}}
\newcommand*{\xpt}[1][]{\mathcal{Y}\ifblank{#1}{}{^{#1}}}
\newcommand{\ones}{\mathbbm{1}}

% Document metadata
\title{An Optimal Interpolation Set for Model-Based Derivative-Free Optimization Methods}
\author{
    Tom M. Ragonneau\thanks{
        Department of Applied Mathematics, The Hong Kong Polytechnic University, Hong Kong.\\
        \email{tom.ragonneau@polyu.edu.hk}; \orcid{0000-0003-2717-2876}.
    } \and
    Zaikun Zhang\thanks{
        Department of Applied Mathematics, The Hong Kong Polytechnic University, Hong Kong.\\
        \email{zaikun.zhang@polyu.edu.hk}; \orcid{0000-0001-8934-8190}. Corresponding author.
    }
}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
    This paper demonstrates the optimality of an interpolation set employed by Powell in derivative-free trust-region methods.
    This set is optimal in the sense that it minimizes the constant of well-poisedness in a ball centred at the starting point.
    It is chosen as the default initial interpolation set by many derivative-free trust-region optimization methods based on underdetermined quadratic interpolation, including \solvername{newuoa}, \solvername{bobyqa}, \solvername{lincoa}, and \solvername{cobyqa}.
    Our analysis provides a theoretical justification for this choice.
\end{abstract}

\keywords{Derivative-free optimization \and Model-based methods \and Underdetermined quadratic interpolation \and Derivative-free symmetric Broyden update \and Well-poisedness}

\msc{90C56 \and 41A10 \and 65K05 \and 90C30}

\section{Introduction}

\Gls{dfo} methods solve the optimization problem
\begin{equation}
    \label{eq:nlp}
    \min_{x \in \fset \subseteq \R^n} \obj(x)\\
\end{equation}
based on function evaluations without relying on derivative information.
They are needed when classical or generalized derivatives of the objective function~$\obj \colon \R^n \to \R$ are unavailable or too expensive to evaluate, which may also be the case for the constraint functions underlying the feasible set~$\fset \subseteq \R^n$.
Examples of such problems arise from many areas, such as reinforcement learning~\cite{Qian_Yu_2021}, hyperparameter tuning~\cite{Ghanbari_Scheinberg_2017}, particle physics~\cite{Eldred_Etal_2022}, and aircraft engineering~\cite{Gazaix_Etal_2019}.
The \gls{dfo} litterature is vast, and for comprehensive overview, we refer to the monographs~\cite{Conn_Scheinberg_Vicente_2009,Audet_Hare_2017}, the survey papers~\cite{Powell_1975,Powell_1998,Rios_Sahinidis_2013,Custodio_Scheinberg_Vicente_2017,Larson_Menickelly_Wild_2019}, the recent thesis~\cite{Ragonneau_2022}, and the references therein.

This paper studies a set employed as the initial interpolation set by many derivative-free trust-region methods based on underdetermined quadratic interpolation, such as \solvername{newuoa}~\cite{Powell_2006}, and, when no constraints are present, \solvername{bobyqa}~\cite{Powell_2009}, \solvername{lincoa}, and \solvername{cobyqa}~\cite{Ragonneau_2022,Ragonneau_Zhang_2023}.
We show that this interpolation set is optimal in the sense that it minimizes the constant of well-poisedness in a ball centred at the starting point, which is detailed in Theorem~\ref{th:optimset}.

% Broadly speaking, \gls{dfo} methods can be divided into two categories: direct-search and model-based methods~\cite{Conn_Scheinberg_Vicente_2009}.
% Direct-search methods~\cite{Kolda_Lewis_Torczon_2003} generate sample points in the search space around the current iterate and select the next iterate among these points based on simple comparisons.
% Examples of direct-search methods include the Hooke-Jeeves method~\cite{Hooke_Jeeves_1961}, the Nelder-Mead method~\cite{Nelder_Mead_1965}, the \solvername{gps} method~\cite{Booker_Etal_1999}, the \solvername{mads} method~\cite{Audet_Dennis_2006}, and \solvername{bfo}~\cite{Porcelli_Toint_2017,Porcelli_Toint_2022}.
% Meanwhile, model-based methods~\cite[Part~4]{Audet_Hare_2017} approximate the objective and constraint functions using simple models around the current iterate and select the next one based on these approximations.
% Since these approximations are local, globalization strategies are needed to ensure global convergence.
% The trust-region framework~\cite{Conn_Gould_Toint_2000} is a popular choice for this purpose and is the one under consideration in this paper, but the line-search framework~\cite[Chapter~3]{Nocedal_Wright_2006} is also applicable~\cite{Berahas_Byrd_Nocedal_2019}.
% Examples of derivative-free trust-region methods include the method of Conn and Toint~\cite{Conn_Toint_1996}, \solvername{mnh}~\cite{Wild_2008}, Powell's methods~\cite{Powell_1994,Powell_2002,Powell_2006,Powell_2009,Ragonneau_Zhang_2021}, and \solvername{cobyqa}~\cite{Ragonneau_2022,Ragonneau_Zhang_2023}.
%
% Model-based \gls{dfo} methods, and particularly derivative-free trust-region methods, often employ polynomial interpolation schemes to approximate the objective and constraint functions.
The \gls{dfo} methods we consider maintain an interpolation set~$\xpt[k] \subseteq \R^n$, where~$k$ denotes the iteration number, and build a quadratic model~$\objm[k]$ of~$\obj$ according to the interpolation conditions
\begin{equation}
    \label{eq:intp-cond}
    \objm[k](y) = \obj(y), \quad y \in \xpt[k].
\end{equation}
% We assume in what follows that the models~$\objm[k]$ are quadratic, and we denote by~$\qpoly$ the space of quadratic polynomials.
In what follows, we denote by~$\qpoly$ the space of polynomials of degree at most two, which will be referred to as quadratic polynomials for simplicity.
% Such models are often employed in practice, for example, by \solvername{uobyqa}~\cite{Powell_2002}, \solvername{newuoa}~\cite{Powell_2006}, \solvername{bobyqa}~\cite{Powell_2009}, \solvername{lincoa}, \solvername{mnh}, and \solvername{cobyqa}.
Clearly, if the interpolation conditions~\eqref{eq:intp-cond} do not contradict each other,~$\objm[k]$ is uniquely defined by~\eqref{eq:intp-cond} only if
\begin{equation*}
    \card(\xpt[k]) = \dim(\qpoly) = \frac{1}{2} (n + 1) (n + 2).
\end{equation*}
This scheme, referred to as fully-determined interpolation, is employed by \solvername{uobyqa}~\cite{Powell_2002}.
However, the first iteration of this method requires~$\bigo(n^2)$ function evaluations to construct the initial model~$\objm[0]$.
This is impracticable in practice unless~$n$ is small.
Therefore, modern interpolation-based \gls{dfo} methods employ fewer interpolation points, giving rise to so-called underdetermined interpolation.
To uniquely define~$\objm[k]$ in this case, it is frequently set to a solution to a problem of the form
\begin{equation}
    \label{eq:intp-var}
    \begin{aligned}
        \min_{Q \in \qpoly} & \quad \mathcal{F}^k(Q)\\
        \text{s.t.}         & \quad Q(y) = \obj(y), \quad y \in \xpt[k],
    \end{aligned}
\end{equation}
where~$\mathcal{F}^k$ is a functional that promotes a desired regularity of~$\objm[k]$.
For example, \solvername{dfo}~\cite{Conn_Scheinberg_Toint_1998} and \solvername{mnh}~\cite{Wild_2008} employ
\begin{equation}
    \label{eq:mnh}
    \mathcal{F}^k(Q) = \norm[\big]{\nabla^2 Q}_{\mathsf{F}}^2,
\end{equation}
where~$\norm{\cdot}_{\mathsf{F}}$ denotes the Frobenius norm, and \solvername{newuoa}, \solvername{bobyqa}, \solvername{lincoa}, and \solvername{cobyqa} employ
\begin{equation}
    \label{eq:df-psb}
    \mathcal{F}^k(Q) = \norm[\big]{\nabla^2 Q - \nabla^2 \objm[k - 1]}_{\mathsf{F}}^2,
\end{equation}
with~$\objm[-1] = 0$.
There are other functionals based on the~$\ell_1$-norm~\cite{Bandeira_Scheinberg_Vicente_2012} or Sobolev seminorms of quadratic polynomials~\cite{Zhang_2014,Xie_Yuan_2022}.
The functional~\eqref{eq:df-psb} is inspired by the least-change property of quasi-Newton updates~\cite{Dennis_Schnabel_1979}.
The variational problem~\eqref{eq:intp-var}--\eqref{eq:df-psb} defines the derivative-free symmetric
Broyden update proposed by Powell (see~\cite{Powell_2004a,Powell_2013},~\cite[\S~3.6]{Zhang_2012}, and~\cite[\S~2.4.2]{Ragonneau_2022}).
To ensure that~$\objm[k]$ is uniquely defined and include second-order information, we assume that
\begin{equation*}
    n + 2 \le \card(\xpt[k]) \le \frac{1}{2} (n + 1) (n + 2).
\end{equation*}
Finally, if the constraint functions underlying the feasible set~$\fset$ in~\eqref{eq:nlp} are nonlinear, we can approximate them using similar techniques, which is the case for \solvername{cobyqa}.

Normally, interpolation-based \gls{dfo} methods do not build the interpolation point~$\xpt[k]$ \emph{ab~initio} at each iteration.
Instead, they choose an initial interpolation set~$\xpt[0]$ and then  update~$\xpt[k]$ to obtain~$\xpt[k + 1]$.
For example, only one point differs between~$\xpt[k]$ and~$\xpt[k + 1]$ in Powell's trust-region \gls{dfo} methods and \solvername{cobyqa} during usual iterations.
This mechanism prevents the methods from performing unnecessary function evaluations, which are normally considered the principal cost of \gls{dfo} methods.
In this case, it is clear that the initial interpolation set~$\xpt[0]$ must be carefully chosen, as it will impact the optimization method for the subsequent iterations.
Powell devised an initial interpolation set~$\xpt[0]$ for his methods, described in Subsection~\ref{subsec:powell-set}.
Although the design of this set is natural, no theoretical analysis has been provided to justify its choice.
This paper contributes such an analysis, showing that no other interpolation set is better in a well-poisedness sense.

This paper is organized as follows.
Section~\ref{sec:well-poisedness} introduces the notion of $\Lambda$-poisedness of interpolation sets in the underdetermined sense.
This notion is then employed in Section~\ref{sec:main-result} to study initial interpolation sets proposed by Powell.
Theorem~\ref{thm:lambda-p}\fxnote{This is not true anymore} is the main result of this paper and provides explicit bounds for the constant of well-poisedness of the interpolation set under study.

\section{Well-poisedness of interpolation sets}
\label{sec:well-poisedness}

In this section, we consider an interpolation set
\begin{equation*}
    \xpt = \set{y^1, y^2, \dots, y^m} \subseteq \R^n
\end{equation*}
and the interpolation problem
\begin{equation}
    \label{eq:intp-cond-gen}
    \begin{aligned}
        \min_{Q \in \qpoly} & \quad \norm{\nabla^2 Q}_\textsf{F}\\
        \text{s.t.}         & \quad Q(y) = \obj(y), \quad y \in \xpt.
    \end{aligned}
\end{equation}
This problem covers~\eqref{eq:intp-var}--\eqref{eq:mnh}, and also covers~\eqref{eq:intp-var} with~\eqref{eq:df-psb} by a simple change of variable.
% Note that if~$\xpt$ is seen as an initial interpolation set~$\xpt[0]$, then~$\objm$ denotes the initial model~$\objm[0]$, and the functional~$\mathcal{F}^k = \mathcal{F}^0$ is equivalent to the one defined by~\eqref{eq:df-psb}.
% In other words,~$\objm$ is also obtained by the derivative-free symmetric Broyden update.

% The following definition guarantees that the interpolation problem is always well-posed and that~$\objm$ exists and is unique.

\begin{definition}[Poisedness]
    The set~$\xpt$ is \emph{poised} in the minimum Frobenius norm sense if problem~\eqref{eq:intp-cond-gen} has a unique solution for any real-valued function~$\obj$.
\end{definition}

% As we mentioned earlier, a necessary condition for~$\xpt$ to be poised is
% \begin{equation*}
%     n + 1 \le m \le \frac{1}{2} (n + 1) (n + 2).
% \end{equation*}
Observe that the variational problem~\eqref{eq:intp-cond-gen} is an equality-constrained quadratic
programming problem with respect to the coefficients of~$Q$. Thus its KKT system is a linear system.
The exact formulation of this system can be found in~\cite{Powell_2004a,Powell_2004b}.
The set~$\xpt$ is poised if this KKT system is uniquely solvable~(see~\mbox{\cite[\S~2]{Powell_2004a}} and~\cite[\S~5.3]{Conn_Scheinberg_Vicente_2009}).
Intuitively, the interpolation set~$\xpt$ can be said well-poised if this system is well-conditioned.
This intuition will be formalized in the sequel.
% To formalize this intuition, we first introduce the notion of minimum-norm Lagrange polynomials.

\subsection{Minimum-norm Lagrange polynomials}

To formally define a measure of well-poisedness of~$\xpt$, we first need to extend the classical definition of the Lagrange polynomials as follows.

\begin{definition}[Minimum-norm Lagrange polynomials~{\cite[Definition~5.1]{Conn_Scheinberg_Vicente_2009}}]
    \label{def:min-norm-lagp}
    Assume that the interpolation set~$\xpt$ is poised.
    The~$i$th \emph{minimum-norm Lagrange polynomial}~$\lagp[i]$ for the interpolation problem~\eqref{eq:intp-cond-gen}, with~$i \in \set{1, 2, \dots, m}$, is the unique quadratic polynomial that solves
    \begin{equation*}
        \begin{aligned}
            \min_{Q \in \qpoly} & \quad \norm[\big]{\nabla^2 Q}_{\mathsf{F}}\\
            \text{s.t.}         & \quad Q(y^j) = \delta_{i, j}, \quad j \in \set{1, 2, \dots, m},
        \end{aligned}
    \end{equation*}
    where~$\delta_{i, j}$ denotes the Kronecker delta.
\end{definition}

Remark that if~$m = (n + 1) (n + 2) / 2$, then Definition~\ref{def:min-norm-lagp} reduces to the classical definition of the Lagrange polynomials.
Morveover, it can be shown that the solution~$\objm$ to problem~\eqref{eq:intp-cond-gen} is given by
\begin{equation*}
    \objm(x) = \sum_{i = 1}^m \obj(y^i) \lagp[i](x)
\end{equation*}
for any~$x \in \R^n$~\cite[Lemma~5.2]{Conn_Scheinberg_Vicente_2009}.

\subsection{Well-poisedness in the minimum Frobenius norm sense}

We are now equipped to define the notion of~$\Lambda$-poisedness in the minimum Frobenius norm sense.

\begin{definition}[$\Lambda$-poisedness in the minimum Frobenius norm sense~{\cite[Definition~5.6]{Conn_Scheinberg_Vicente_2009}}]
    \label{def:lambda-p}
    The poised interpolation set~$\xpt$ is said to be~\emph{$\Lambda$-poised in the minimum Frobenius norm sense} in a compact set~$\mathcal{C} \subseteq \R^n$, for some~$\Lambda > 0$, if
    \begin{equation*}
        \Lambda \ge \max_{1 \le i \le m} \max_{x \in \mathcal{C}} @@ \abs[\big]{\lagp[i](x)}.
    \end{equation*}
\end{definition}

If~$\xpt$ is~$\Lambda_0$-poised in the minimum Frobenius norm sense in~$\mathcal{C}$, it is obviously~$\Lambda$-poised in same sense for all~$\Lambda \ge \Lambda_0$.
Moreover, one can show that~$\xpt$ is~$\Lambda$-poised if and only if the condition number of the coefficient matrix of the KKT system of~\eqref{eq:intp-var} is bounded by some terms proportional to~$\Lambda$~\cite[Theorem~5.8]{Conn_Scheinberg_Vicente_2009}.
Therefore, the notion of~$\Lambda$-poisedness formalizes the intuitive notion of well-poisedness mentioned above.

\section{Optimality of Powell's initial interpolation set}
\label{sec:main-result}

% Recall that Powell proposed an initial interpolation set~$\xpt[0]$ that is employed by many interpolation-based \gls{dfo} methods, particularly trust-region methods, including \solvername{uobyqa}, \solvername{newuoa}, and also \solvername{bobyqa}, \solvername{lincoa}, and \solvername{cobyqa} if there are no constraints.
This section first presents the initial interpolation set devised by Powell for his trust-region \gls{dfo} methods, and then analyzes its well-poisedness.
We establish bounds on its constant of well-poisedness, and evaluate this constant in some special cases.
Finally, we point out that the default setting in Powell's methods renders an optimal interpolation set in terms of the well-poisedness in a ball centred at the starting point.

\subsection{Description of the initial interpolation set}
\label{subsec:powell-set}

The initial interpolation that Powell designed for his methods is as follows.
Without loss of generality, we assume that the starting point is at the origin.
Suppose that~$\Delta > 0$ is the initial trust-region radius.
For~$i \in \set{1, 2, \dots, 2n + 1}$, define
\begin{equation}
    \label{eq:set-def}
    y^i =
    \begin{cases}
        0,                      & \text{if~$i = 1$,}\\
        \Delta e_{i - 1},       & \text{if~$2 \le i \le n + 1$,}\\
        -\Delta e_{i - n - 1},  & \text{otherwise},
    \end{cases}
\end{equation}
where~$e_i \in \R^n$ denotes the $i$th canonical coordinate vector in~$\R^n$.
Let~$m$ be the number of interpolation points. We focus on the case that~$n+2 \le m \le 2n+1$
following Powell's suggestion.\footnote{
Powell's \solvername{bobyqa} code contains a comment that `choices that exceed $2n+1$ are not recommended'.
}
The initial interpolation set is then chosen to be
\begin{equation}
    \label{eq:set-def-m}
    \xpt[0]_m = \set{y^1, y^2, \dots, y^m},
\end{equation}
which can be found in~\cite[eq.~(3.2)]{Powell_2006}.
The default value for~$m$ proposed by Powell is~$2n + 1$, i.e., the default initial interpolation
set is~$\xpt[0] = \xpt[0]_{2n + 1}$.
This is a natural choice, as the corresponding interpolation set is geometrically appealing.
It consists of the origin and equidistant points to the origin in the positive and negative coordinate directions.
In what follows, we provide a theory that justifies this natural choice, showing that~$\xpt[0]_{2n + 1}$
is indeed optimal in terms of well-poisedness.

\subsection{Well-poisedness of the interpolation set}

We now investigate the $\Lambda$-poisedness~$\xpt[0]_m$ for different values of~$m \in \{n+2, n+3, \dots, 2n+1\}$.
We will do this in the closed~$\ell_p$-norm ball of radius~$\Delta$ centred at the origin, namely
\begin{equation*}
    \mathcal{B}_p(\Delta) = \set{x \in \R^n : \norm{x}_p \le \Delta},
\end{equation*}
with~$p \in [1, \infty]$.
Note that we allow~$p = \infty$, and that~$\mathcal{B}_p(\Delta)$ is also the smallest~$\ell_p$-norm ball enclosing~$\xpt[0]_m$.
Powell's methods define the trust region by the Euclidean norm, so the case with~$p=2$ is the most
interesting. However, it can be beneficial to define the trust region by polyhedral norms when bound
or linear constraints are present, and $p=1$ or~$p=\infty$ will become more relevant.

According to Definition~\ref{def:lambda-p}, the set~$\xpt[0]_m$ is~$\Lambda_p$-poised in~$\mathcal{B}_p(\Delta)$ in the minimum Frobenius norm sense~with
\begin{equation}
    \label{eq:Lambdap}
    \Lambda_p = \max_{1 \le i \le m} \max_{\norm{x}_p \le \Delta} \abs[\big]{\lagp[i](x)},
\end{equation}
where~$\lagp[i]$ is the~$i$th minimum-norm Lagrange polynomial associated with~$\xpt[0]_m$ for~$i \in \set{1, 2, \dots, m}$.
We refer to~$\Lambda_p$ as the \emph{constant of well-poisedness} of~$\xpt[0]_m$ in~$\mathcal{B}_p(\Delta)$.

\subsubsection{Formulation of the Lagrange polynomials}

To study~$\Lambda_p$, we first present explicit formulae for~$\lagp[i]$ for all~$i \in \set{1, 2, \dots, m}$.
These formulae are given in~\cite[\S~3]{Powell_2006}, without a proof.

\begin{lemma}
    \label{lem:lagp}
    For each~$m \in \set{n + 2, n + 3, \dots,  2n + 1}$ and all~$x \in \R^n$, we have
    \begin{equation*}
        \lagp[i](x) =
        \begin{cases}
            \displaystyle 1 - \frac{1}{\Delta^2} \sum_{j = 1}^{m - n - 1} \!\! x_j^2 - \frac{1}{\Delta} \sum_{j = m - n}^n \!\! x_j,    & \text{if~$i = 1$,}\\[2ex]
            \displaystyle \frac{x_{i - 1}^2}{2\Delta^2} + \frac{x_{i - 1}}{2\Delta},                                                    & \text{if~$2 \le i \le m - n$,}\\[2ex]
            \displaystyle \frac{x_{i -n - 1}^2}{2\Delta^2} - \frac{x_{i -n- 1}}{2\Delta},                                               & \text{if~$n+2\le i \le m$,}\\[2ex]
            \displaystyle \frac{x_{i - 1}}{\Delta},                                                                                     & \text{if~$m - n + 1 \le i \le n + 1$.}
            %& \text{otherwise.}
        \end{cases}
    \end{equation*}
    Here, $x_j$ denotes the~$j$th entry of~$x$ for each~$j\in\{1,2, \dots, n\}$, and we define~$\sum_{j = m - n}^n x_j = 0$ in the
    formulation of~$\lagp[1]$ if~$m = 2n+1$.
\end{lemma}

\begin{proof}
    Let~$i \in \set{1, 2, \dots, m}$ be fixed and let~$\lagp$ be a quadratic polynomial that satisfies
    \begin{equation}
        \label{eq:lagp-p}
        \lagp(y^j) = \delta_{i, j}, \quad j \in \set{1, 2, \dots, m}.
    \end{equation}
    First, it is straightforward to verify that~$\lagp[i]$ satisfies the interpolation conditions~\eqref{eq:lagp-p}.
    Hence, it suffices to show that~$\|\nabla^2 \lagp[i]\|_\mathsf{F} \le \|\nabla^2 \lagp\|_\mathsf{F}$.

    Consider any~$j\in\{1,2,\dots, m-n-1\}$.
    Denote the $j$th diagonal entries of~$\nabla^2 \lagp$ and~$\nabla^2 \lagp[i]$ by~$(\nabla^2\lagp)_{j,j}$ and~$(\nabla^2\lagp[i])_{j,j}$, respectively.
    According to equation~\eqref{eq:set-def}, we have
    \begin{equation*}
        %\label{eq:}
        y^1 = 0, \quad y^{j + 1} = \Delta e_j, \quad \text{and} \quad~y^{n + j + 1} = -\Delta e_j.
    \end{equation*}
    Since~$\lagp$ and~$\lagp[j]$ are quadratic polynomials sharing the same values on~$\{y^1, y^{j+1},
    y^{n+j+1}\}$, we have
    \begin{equation*}
        \big(\nabla^2 \lagp \big)_{j,j}
        = \frac{\lagp(y^{j + 1}) + \lagp(y^{n + j + 1}) - 2 \lagp(y^1)}{\Delta^2}
        = \big(\nabla^2 \lagp[i] \big)_{j,j}.
    \end{equation*}
    On the other hand, it is easy to check that all the entries of~$\nabla^2 \lagp[i]$ are zero
    except for the first~$m-n-1$ diagonal entries. Therefore,
    \begin{equation*}
        \norm[\big]{\nabla^2 \lagp[i]}_{\mathsf{F}}^2 \le \norm[\big]{\nabla^2 \lagp}_{\mathsf{F}}^2,
    \end{equation*}
    which completes the proof.
\end{proof}

\subsubsection{Bounds for the~\texorpdfstring{$\Lambda$}{\textLambda}-poisedness}

The next lemma simplifies the expression of~$\Lambda_p$ defined in~\eqref{eq:Lambdap} for further computations.

\begin{lemma}
    \label{lem:lambda-p}
    For any~$m \in \set{n + 2, n + 3, \dots, 2n + 1}$ and any~$p \in [1, \infty]$, we have
    \begin{equation}
        \label{eq:lambda-p}
        \Lambda_p = \max_{\norm{x}_p \le \Delta} \abs[\big]{\lagp[1](x)}.
    \end{equation}
\end{lemma}

\begin{proof}
    According to Lemma~\ref{lem:lagp}, for each~$i \in \set{2, 3, \dots, n + 1}$,~$\lagp[i](x)$ only depends on~$x_{i - 1}$ for all~$x \in \R^n$, and hence
    \begin{equation*}
        \max_{\norm{x}_p \le \Delta} \abs[\big]{\lagp[i](x)} = \max_{t \in [-\Delta, \Delta]} \abs[\big]{\lagp[i](t e_{i - 1})} = 1.
    \end{equation*}
    Similarly, for each~$i \in \set{n + 2, n + 3, \dots, m}$, since~$\lagp[i](x)$ only depends on~$x_{i - n - 1}$ for all~$x \in \R^n$, we have
    \begin{equation*}
        \max_{\norm{x}_p \le \Delta} \abs[\big]{\lagp[i](x)} = \max_{t \in [-\Delta, \Delta]} \abs[\big]{\lagp[i](t e_{i - n - 1})} = 1.
    \end{equation*}
    Meanwhile, since~$\lagp[1](y^1) = 1$ and~$y^1 \in \mathcal{B}_p(\Delta)$, we have
    \begin{equation*}
        \max_{\norm{x}_p \le \Delta} \abs[\big]{\lagp[1](x)} \ge \lagp[1](y^1) = 1.
    \end{equation*}
    Hence~\eqref{eq:lambda-p} holds according to the definition of~$\Lambda_p$ in~\eqref{eq:Lambdap}.
\end{proof}

We are now equipped to develop bounds on~$\Lambda_p$ in the general case.

\begin{theorem}
    \label{thm:lambda-p}
    For any~$m \in \set{n + 2, n + 3, \dots, 2n + 1}$ and any~$p \in [1, \infty]$, we have
    \begin{equation*}
        1 + (2n + 1 - m)^{\frac{p - 1}{p}} \le \Lambda_p \le n,
    \end{equation*}
    where we define~$0^0 = 0$ and~$\infty/\infty = 1$.% the lower bound is~$2n + 2 - m$ for~$p = \infty$.
\end{theorem}

\begin{proof}
    We will establish the bounds using the formulation of~$\Lambda_p$ in Lemma~\ref{lem:lambda-p}.
    For the lower bound, by considering only the points in~$\R^n$ whose leading~$m - n - 1$ entries are zeros and whose remaining~$2n + 1 - m$ entries are equal, we have
    \begin{equation*}
        \Lambda_p \ge \max_{t \in \R} \set[\bigg]{1 - \frac{1}{\Delta} (2n + 1 - m) t : (2n + 1 - m) \abs{t}^p \le \Delta^p} = 1 + (2n + 1 - m)^{\frac{p - 1}{p}}.
    \end{equation*}
    We now establish the upper bound.
    For any~$p \ge 1$, we have~$\mathcal{B}_p(\Delta) \subseteq \mathcal{B}_{\infty}(\Delta)$, so that~$\Lambda_p \le \Lambda_{\infty}$.
    Therefore, we only need to show that~$\Lambda_{\infty} \le n$.
    This is true because
    \begin{equation*}
        \Lambda_{\infty} =
        \max_{\norm{x}_{\infty} \le \Delta} \abs[\big]{\lagp[1](x)}
        = \max\bigg\{\max_{\norm{x}_{\infty} \le \Delta} \lagp[1](x), \,\max_{\norm{x}_{\infty} \le \Delta} -\lagp[1](x) \bigg\}
        = \max \set{2n + 2 - m, n - 1} \le n.
    \end{equation*}
\end{proof}

\subsubsection{Some special cases}

We now calculate the exact value of~$\Lambda_p$ in some special cases.

\begin{proposition}
    \label{prop:lambda-p-1}
    For any~$m \in \set{n + 2, n + 3, \dots, 2n + 1}$, we have
    \begin{equation}
        \label{eq:lambda-p-1}
        \Lambda_1 =
        \begin{cases}
            2,  & \text{if~$n + 2 \le m \le 2n$,}\\
            1,  & \text{if $m = 2n+1$.}
        \end{cases}
    \end{equation}
\end{proposition}

\begin{proof}
    According to Theorem~\ref{thm:lambda-p},~$\Lambda_1$ is lower bounded by the right-hand side of~\eqref{eq:lambda-p-1}.
    Therefore, we only need to prove that this right-hand side is also an upper bound for~$\Lambda_1$, using Lemma~\ref{lem:lambda-p}.

    For any~$x \in \mathcal{B}_1(\Delta)$, according to Lemma~\ref{lem:lagp}, we have
    \begin{equation*}
        \lagp[1](x) \le 1 - \frac{1}{\Delta} \sum_{j = m - n}^n \!\! x_j \le 1 + \frac{1}{\Delta} \sum_{j = m - n}^n \!\! \abs{x_j}.
    \end{equation*}
    Therefore,
    \begin{equation}
        \label{eq:lambda-p-1-p-1}
        \lagp[1](x) \le
        \begin{cases}
            2,  & \quad \text{if~$n + 2 \le m \le 2n$,}\\
            1,  & \quad \text{if~$m = 2n+1$.}
        \end{cases}
    \end{equation}
    On the other hand,
    \begin{equation}
        \label{eq:lambda-p-1-p-2}
        \lagp[1](x) = 1 - \sum_{j = 1}^{m - n - 1} \frac{x_j^2}{\Delta^2} - \sum_{j = m - n}^n \frac{x_j}{\Delta} \ge 1 - \sum_{j = 1}^{m - n - 1} \frac{\abs{x_j}}{\Delta} - \sum_{j = m - n}^n \frac{\abs{x_j}}{\Delta} \ge 1 - \frac{\norm{x}_1}{\Delta} \ge 0.
    \end{equation}
    We conclude the proof by combining~\eqref{eq:lambda-p-1-p-1} and~\eqref{eq:lambda-p-1-p-2} with Lemma~\ref{lem:lambda-p}.
\end{proof}

\begin{proposition}
    \label{prop:lambda-p-2}
    For any~$m \in \set{n + 2, n + 3, \dots, 2n + 1}$, we have
    \begin{equation}
        \label{eq:lambda-p-2}
        \Lambda_2 = 1 + \sqrt{2n + 1 - m}.
    \end{equation}
\end{proposition}

\begin{proof}
    %If~$m = 2n + 1$, Lemma~\ref{lem:lagp} tells us that~$\lagp[1](x) = 1 - \norm{x}_2^2 / \Delta^2$ for~$x \in \mathcal{B}_2(\Delta)$.
    %Therefore, Lemma~\ref{lem:lambda-p} provides the desired result~$\Lambda_2 = 1$.
    %We now focus on the case with~$n + 2 \le m < 2n + 1$.
    According to Theorem~\ref{thm:lambda-p},~$\Lambda_2$ is lower bounded by the right-hand side of~\eqref{eq:lambda-p-2}.
    Therefore, we only need to prove that this right-hand side is also an upper bound for~$\Lambda_2$, using Lemma~\ref{lem:lambda-p}.

    For any~$x \in \mathcal{B}_2(\Delta)$, according to Lemma~\ref{lem:lagp} and the Cauchy-Schwartz inequality, we have
    \begin{align}
        -\lagp[1](x) &= -1 + \frac{1}{\Delta^2} \sum_{j = 1}^{m - n - 1} \!\! x_j^2 + \frac{1}{\Delta} \sum_{j = m - n}^n \!\! x_j \nonumber\\
                    &\le -1 + \frac{1}{\Delta^2}\norm{x}_2^2 + \frac{1}{\Delta} \sqrt{2n+1-m}\,\norm{x}_2 \nonumber \\
                    & \le \sqrt{2n+1-m} \nonumber. % \label{eq:lambda-p-2-p}
    \end{align}
    Similarly,
    \begin{equation*}
        \lagp[1](x) \le 1 + \frac{1}{\Delta}\sum_{j = m - n}^n \abs{x_j} \le 1 + \frac{1}{\Delta} \sqrt{2n + 1 - m} \,\norm{x}_2 \le 1 + \sqrt{2n + 1 - m}.
    \end{equation*}
    Therefore, according to Lemma~\ref{lem:lambda-p}, we have
    \begin{equation*}
        \Lambda_2 = \max_{\norm{x}_2 \le \Delta} \abs[\big]{\lagp[1](x)} \le 1 + \sqrt{2n + 1 - m},
    \end{equation*}
    which concludes the proof.
\end{proof}

Before developing the last special value of~$\Lambda_p$ we introduce the following lemma.

\begin{lemma}
    \label{lem:max-norm-pq}
    For any~$p \ge 1$ and~$q \ge 1$, we have
    \begin{subequations}
        \begin{numcases}{\max_{\norm{x}_q \le 1} @ \norm{x}_p =}
            1,                      & \text{if~$p \ge q$,} \label{eq:max-norm-pq-1}\\
            n^{\frac{q - p}{pq}},   & \text{otherwise.} \label{eq:max-norm-pq-2}
        \end{numcases}
    \end{subequations}
\end{lemma}

\begin{proof}
    Let us first consider the case where~$p \ge q$.
    For~$x \in \mathcal{B}_q(1)$, we have~$\norm{x}_p \le 1$, and this bound is attained at the first coordinate vector~$e_1 \in \mathcal{B}_q(1)$, so that~\eqref{eq:max-norm-pq-1} holds.

    We now consider the case where~$p < q$.
    Let~$\ones \in \R^n$ denote the vector with all entries being one,~$r = q/p$, and~$s = r / (r - 1) = q / (q - p)$.
    For~$x \in \mathcal{B}_q(1)$, define~$z = (\abs{x_1}^p, \abs{x_2}^p, \dots, \abs{x_n}^p)$.
    According to the H{\"{o}}lder inequality, we have
    \begin{equation*}
        \norm{x}_p  = \big(\ones^{\T} z \big)^{\frac{1}{p}} \le \big(\norm{\ones}_s \norm{z}_r \big)^{\frac{1}{p}} = n^{\frac{q - p}{pq}} \norm{x}_q \le n^{\frac{q - p}{pq}}.
    \end{equation*}
    Moreover, this bound is attained at~$x^\ast = n^{-\frac{1}{q}}\ones$, which proves~\eqref{eq:max-norm-pq-2}.
\end{proof}

\begin{proposition}
    \label{prop:lambda-p-opt}
    For any~$p \ge 1$, if~$m = 2n + 1$, then
    \begin{equation*}
        \Lambda_p = \max \set[\big]{1, n^{\frac{p - 2}{p}} - 1}.
    \end{equation*}
\end{proposition}

\begin{proof}
    Since~$m = 2n+1$, we have~$L_1(x) = 1- \Delta^{-2}\norm{x}_2^2$ according to Lemma~\ref{lem:lagp}.
    Then it is clear that
    \begin{equation}
        \label{eq:lambda-p-inf-p-1}
        \max_{\norm{x}_p \le \Delta} \lagp[1](x) = \max_{\norm{x}_p \le \Delta} \big( 1 - \Delta^{-2}{\norm{x}_2^2} \big) = 1.
    \end{equation}
    Moreover, according to Lemma~\ref{lem:max-norm-pq}, we have
    \begin{equation}
        \label{eq:lambda-p-inf-p-2}
        \max_{\norm{x}_p \le \Delta} -\lagp[1](x) = \max_{\norm{x}_p \le \Delta} \big( \Delta^{-2} {\norm{x}_2^2} - 1 \big) =
        \begin{cases}
            0,                          & \quad \text{if~$p \le 2$,}\\
            n^{\frac{p - 2}{p}} - 1,    & \quad \text{otherwise.}
        \end{cases}
    \end{equation}
    The desired result is obtained by combining~\eqref{eq:lambda-p-inf-p-1} and~\eqref{eq:lambda-p-inf-p-2} with Lemma~\ref{lem:lambda-p}.
\end{proof}

Now we can show that~$\xpt[0]_{2n+1}$ attains the minimal constant of well-poisedness in~$\mathcal{B}_p(\Delta)$ among all interpolation sets that contain the origin, provided that~$1 \le p\le 2$.
In this sense,~$\xpt[0]_{2n + 1}$ is an optimal interpolation set in~$\mathcal{B}_p(\Delta)$ for~$p \in [1, 2]$.
Indeed, the upper bound for~$p$ can be slightly larger than~$2$, as is detailed in Theorem~\ref{th:optimset}.
Recall that~$p=2$ is of interest concerning Powell's methods.
\begin{theorem}
    \label{th:optimset}
    Assume that~$m=2n+1$, and that either~$n\le 2$ or
    \begin{equation}
        \label{eq:pineq}
        1 \le p \le \frac{2\log n}{\log (n/2)}.
    \end{equation}
    If an interpolation set containing~$0$ is~$\Lambda$-poised in~$\mathcal{B}_p(\Delta)$, then~$\Lambda \ge \Lambda_p$.
\end{theorem}

\begin{proof}
    Under the assumptions, we have~$\Lambda_p = 1$ according to Proposition~\ref{prop:lambda-p-opt}.
    If an interpolation set containing~$0$ is~$\Lambda$-poised in~$\mathcal{B}_p(\Delta)$, then we
    have~$\Lambda \ge 1$, because the Lagrange polynomial corresponding to~$0$ takes the value~$1$ at~$0$.
    Thus the theorem holds.
\end{proof}
It is worth mentioning that Propositions~\ref{prop:lambda-p-1} and~\ref{prop:lambda-p-2} also imply that~$\xpt[0]_{2n+1}$ renders~$\Lambda_p = 1$ for~$p \in \set{1, 2}$.
This equality turns out to be true for a larger range of~$p$.



\subsubsection{Remarks and open questions}

Note that~$\Lambda_p$ can be regarded as a function of~$m$. Theorem~\ref{th:optimset} implies that~$m^\ast = 2n+1$ minimizes this function if $p$ satisfies inequality~\eqref{eq:pineq}.
We conjecture that~$2n+1$ minimizes~$\Lambda_p$ for any~$p \ge 1$, but we do not have any proof for such a statement yet.

The definition of~$\xpt[0]_m$ in~\eqref{eq:set-def} and~\eqref{eq:set-def-m} assumes that~$m \le 2n + 1$.
Even though larger values of~$m$ are not recommended in practice, Powell~\cite{Powell_2006} proposed
an extension of~$\xpt[0]_m$ for~$m>2n+1$.
With such an extension, we can define~$\Lambda_p$ by~\eqref{eq:Lambdap} for any~$m \in \{n+2, n+3, \dots, (n+1)(n+2)/2\}$.
It is interesting to ask whether~$2n + 1$ still minimizes~$\Lambda_p$ for any~$p \ge 1$, which seems to be true according to some numerical experiments.
We leave this problem open and expect the analysis to be more challenging than what we have done.
One of the challenges is that Lemma~\ref{lem:lambda-p} does not hold when~$m > 2n + 1$, and hence, the estimations of~$\Lambda_p$ will become more involving.

The spirit of our analysis is similar to that of~\cite{Dodangeh_Vicente_Zhang_2016}, which proves that a widely used direction set is indeed optimal for directional direct search methods
based on sufficient decrease. While proving nothing surprising, this kind of investigation deepens our understanding of certain algorithmic strategies that we often employ but rarely ask why.


\section{Conclusion}

%This paper demonstrated that a natural choice made by Powell when designing \solvername{newuoa} is supported by the interpolation theory.
%In particular, we showed that the interpolation set he proposed for \solvername{newuoa} is optimal in the sense that it minimizes its constant of well-poisedness in a ball centred at the starting point.
We have analyzed the well-poisedness of an interpolation set employed in many trust-region \gls{dfo} methods, particularly those by Powell.
This set is proved to be optimal under the default setting recommended by Powell, because it minimizes the constant of well-poisedness in a ball centred at the starting point.
Our analysis justifies the natural configuration of this set from the viewpoint of interpolation theory.

\paragraph*{Acknowledgement}
This paper corresponds to Section~2.5 of the first author's Ph.D.~thesis~\cite{Ragonneau_2022}, co-supervised by the second author and Professor Xiaojun Chen from The Hong Kong Polytechnic University.
Both authors are very grateful to Professor Chen for her support, encouragement, and guidance during the thesis.

\paragraph*{Disclosure statement}
The authors report that there are no competing interests to declare.

\paragraph*{Funding}
This work was supported by the University Grants Committee of Hong Kong under grants PF18-24698 (Hong Kong Ph.D. Fellowship Scheme), PolyU 253012/17P, PolyU 153054/20P, and PolyU 153066/21P, and by The Hong Kong Polytechnic University under grant P0009767.

\bibliographystyle{amsplain}
\bibliography{\jobname}

\end{document}
