%% Makefile
%% Copyright 2023 Tom M. Ragonneau and Zaikun Zhang
\documentclass[draft]{article}
\usepackage[utf8]{inputenc}
\usepackage[a4paper]{geometry}
\usepackage{microtype}

% Font and font encoding
\usepackage{lmodern}
\usepackage[T1]{fontenc}

% Line spacing
\usepackage[nodisplayskipstretch]{setspace}
\onehalfspacing

% Font Awesome 5
\usepackage{fontawesome5}
\usepackage[dvipsnames]{xcolor}
\definecolor{ORCIDGreen}{HTML}{A6CE39}

% Cross-referencing
\usepackage{textgreek}
\usepackage[
    pdfusetitle,
    hyperfootnotes=false,
]{hyperref}
\usepackage{url}
\hypersetup{
    final,
    colorlinks=true,
    linktoc=page,
    linkcolor=OliveGreen,
    anchorcolor=black,
    citecolor=MidnightBlue,
    filecolor=black,
    menucolor=black,
    runcolor=black,
    urlcolor=black,
}
\newcommand{\email}[2][inbox]{\faIcon{#1} \href{mailto:#2}{#2}}
\newcommand{\phone}[2][phone]{\faIcon{#1} \href{tel:#2}{#2}}
\newcommand{\orcid}[1]{\texorpdfstring{\hspace{1.5ex}\href{https://orcid.org/#1}{\textcolor{ORCIDGreen}{\faIcon{orcid}}}}{}}
\newcommand{\website}[2][globe]{\faIcon{#1} \href{https://#2/}{#2}}

% Glossaries
\usepackage[acronym]{glossaries}
\glsdisablehyper
\newacronym{dfo}{DFO}{derivative-free optimization}

% Mathematics
\usepackage{mathtools}
\usepackage{cases}
\usepackage[bb=dsfontserif]{mathalfa}
\numberwithin{equation}{section}

% Line numbering in draft mode
\usepackage{lineno}
\usepackage{ifdraft}
\renewcommand{\linenumberfont}{\normalfont\scriptsize\sffamily\color{gray}}
\ifdraft{\linenumbers}{}

% Show labels in draft mode
\ifdraft{}{\PassOptionsToPackage{final}{showlabels}}
\usepackage{showlabels}
\renewcommand{\showlabelfont}{\linespread{1}\scriptsize\sffamily\color{gray}}

% To-do notes in draft mode
\usepackage[
    marginface=\linespread{1}\scriptsize\sffamily\color{gray},
    author={},
]{fixme}

% Modify the typesets of the `\maketitle' and `\thanks' commands
\usepackage{titling}
\setlength{\thanksmargin}{0pt}

% Page headers and footers
\usepackage{fancyhdr}
\usepackage{lastpage}
\fancypagestyle{plain}{
    \fancyhf{}
    \renewcommand{\headrulewidth}{0pt}
    \fancyfoot[C]{Page \thepage\ of \pageref*{LastPage}}
}
\pagestyle{plain}

% Exceptions for American English hyphenation patterns
\input{ushyphex}

% Mathematical environments (theorems, proofs, ...)
\usepackage{amsthm}
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\theoremstyle{plain}
\newtheorem{proposition}{Proposition}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]

% Control sequences, macros, and definitions
\usepackage{etoolbox}
\usepackage{xspace}
\DeclareMathOperator{\bigo}{\mathcal{O}}
\DeclareMathOperator{\card}{card}
\newcommand{\abs}[2][]{#1\lvert#2#1\rvert}
\newcommand{\fset}{\Omega}
\newcommand{\lagp}[1][]{L\ifblank{#1}{}{_{#1}}}
\newcommand{\norm}[2][]{#1\lVert#2#1\rVert}
\newcommand{\obj}{f}
\newcommand{\objm}[1][]{\hat{\obj}\ifblank{#1}{}{^{#1}}}
\newcommand{\qpoly}{\mathcal{Q}_n}
\newcommand{\R}{\mathbb{R}}
\newcommand{\set}[2][]{#1\{#2#1\}}
\newcommand{\solvername}[1]{\textsc{#1}\xspace}
\newcommand{\T}{\mathsf{T}}
\newcommand{\xpt}[1][]{\mathcal{Y}\ifblank{#1}{}{^{#1}}}

% Document metadata
\title{An Optimal Interpolation Set for Model-Based Derivative-Free Optimization Methods}
\author{
    Tom M. Ragonneau\thanks{
        Department of Applied Mathematics, The Hong Kong Polytechnic University, Hong Kong\\
        \email{tom.ragonneau@polyu.edu.hk} \quad \website[globe-asia]{www.tomragonneau.com}
    }\orcid{0000-0003-2717-2876} \and
    Zaikun Zhang\thanks{
        Department of Applied Mathematics, The Hong Kong Polytechnic University, Hong Kong\\
        \email{zaikun.zhang@polyu.edu.hk} \quad \website[globe-asia]{www.zhangzk.net}
    }\orcid{0000-0001-8934-8190}
}
\date{\today}
\hypersetup{
    pdfsubject={},
    pdfkeywords={},
}

\begin{document}

\maketitle

\begin{abstract}
    \fxnote*{Write down the abstract}{To do.}
\end{abstract}

\paragraph*{Keywords}
Derivative-free optimization $\cdot$ Model-based methods $\cdot$ Underdetermined quadratic interpolation $\cdot$ Derivative-free symmetric Broyden update $\cdot$ $\Lambda$-poisedness

\paragraph*{Mathematics Subject Classification (2010)}
90C56 $\cdot$ 41A10 $\cdot$ 65K05 $\cdot$

\section{Introduction}

It is challenging to develop \gls{dfo} methods, i.e., optimization methods that do not rely on derivatives of the problems they solve.
However, such methods are required to solve the optimization problem
\begin{equation}
    \label{eq:nlp}
    \begin{aligned}
        \min_{x \in \R^n}   & \quad \obj(x)\\
        \text{s.t.}         & \quad x \in \fset,
    \end{aligned}
\end{equation}
when classical or generalized derivatives of the objective function~$\obj : \R^n \to \R$ (and possibly the constraint functions underlying the feasible~$\fset \subseteq \R^n$) are unavailable or too expensive to evaluate.
There are many examples of applications of this kind, such as machine learning~\cite{Ghanbari_Scheinberg_2017}, reinforcement learning~\cite{Qian_Yu_2021}, particle physics~\cite{Eldred_Etal_2022}, and aircraft engineering~\cite{Gazaix_Etal_2019}.

Broadly speaking, \gls{dfo} methods can be divided into two categories: direct-search and model-based methods~\cite{Conn_Scheinberg_Vicente_2009}.
Direct-search methods~\cite{Kolda_Lewis_Torczon_2003} generate sample points in the search space around the current iterate, and select the next iterate among these points based on simple comparisons.
Examples of famous direct-search methods include the Hooke-Jeeves method~\cite{Hooke_Jeeves_1961}, the Nelder-Mead method~\cite{Nelder_Mead_1965}, the \solvername{gps} method~\cite{Booker_Etal_1999}, the \solvername{mads} method~\cite{Audet_Dennis_2006}, and \solvername{bfo}~\cite{Porcelli_Toint_2017,Porcelli_Toint_2022}.
Meanwhile, model-based methods~\cite[Part~4]{Audet_Hare_2017} approximate the objective and constraint functions by simple models around the current iterate, and select the next iterate by examining these approximations.
Since these approximations are local, the methods are embedded in globalization strategies to ensure convergence.
The trust-region framework~\cite{Conn_Gould_Toint_2000} is a popular choice for this purpose, and is the one under consideration in this paper.
Examples of well-known trust-region methods include the method of Conn and Toint~\cite{Conn_Toint_1996}, \solvername{mnh}~\cite{Wild_2008}, Powell's \gls{dfo} methods~\cite{Powell_1994,Powell_2002,Powell_2006,Powell_2009}, and \solvername{cobyqa}~\cite{Ragonneau_2022,Ragonneau_Zhang_2023}.
% There also exist hybrid methods that combine the two approaches, such as implicit filtering~\cite{Kelley_2011}.

Model-based \gls{dfo} methods and in particular trust-region \gls{dfo} methods often employ polynomial interpolation schemes to approximate the objective and constraint functions.
% In particular, this is the case for the Powell's \gls{dfo} methods and \solvername{cobyqa}.
These methods maintain an interpolation set~$\xpt[k] \subseteq \R^n$, where~$k$ denotes the iteration number, and build a model~$\objm[k]$ of~$\obj$ according to the interpolation conditions
\begin{equation}
    \label{eq:intp-cond}
    \objm[k](y) = \obj(y), \quad y \in \xpt[k].
\end{equation}
% Note that the methods must ensure that the interpolation conditions~\eqref{eq:intp-cond} do not contradict each others.
We assume in what follows that the models~$\objm[k]$ are quadratic, and we denote by~$\qpoly$ the space of quadratic polynomials.
Such models are often employed in practice, for example by \solvername{uobyqa}~\cite{Powell_2002}, \solvername{newuoa}~\cite{Powell_2006}, \solvername{bobyqa}~\cite{Powell_2009}, \solvername{lincoa}, \solvername{mnh}, and \solvername{cobyqa}.
Clearly, if the interpolation conditions~\eqref{eq:intp-cond} do not contradict each others and
\begin{equation*}
    \card\xpt[k] = \dim\qpoly = \frac{1}{2} (n + 1) (n + 2),
\end{equation*}
then~$\objm[k]$ is uniquely defined by~\eqref{eq:intp-cond}.
This scheme, referred to as \emph{fully-determined interpolation}, is the one employed by \solvername{uobyqa}.
However, the first iteration of this method requires~$\bigo(n^2)$ function evaluations to construct the initial model~$\objm[0]$.
This is impracticable in practice unless~$n$ is small.
Therefore, modern interpolation-based \gls{dfo} methods employ fewer interpolation points, giving rise to so-called \emph{underdetermined interpolation}.
To uniquely define~$\objm[k]$ in this case, it is frequently set to a solution to a variational problem of the form
\begin{equation}
    \label{eq:intp-var}
    \begin{aligned}
        \min_{Q \in \qpoly} & \quad \mathcal{F}^k(Q)\\
        \text{s.t.}         & \quad Q(y) = \obj(y), \quad y \in \xpt[k],
    \end{aligned}
\end{equation}
where~$\mathcal{F}^k$ is a functional that promotes a desired regularity of~$\objm[k]$.
For example, \solvername{mnh} employs
\begin{equation}
    \label{eq:mnh}
    \mathcal{F}^k(Q) = \norm[\big]{\nabla^2 Q}_{\mathsf{F}}^2,
\end{equation}
where~$\norm{\cdot}_{\mathsf{F}}$ denotes the Frobenius norm, and \solvername{newuoa}, \solvername{bobyqa}, \solvername{lincoa}, and \solvername{cobyqa} employ
\begin{equation}
    \label{eq:df-psb}
    \mathcal{F}^k(Q) = \norm[\big]{\nabla^2 Q - \nabla^2 \objm[k - 1]}_{\mathsf{F}}^2,
\end{equation}
with~$\objm[-1] = 0$.
The latter functional is inspired by least-change property of quasi-Newton updates~\cite{Dennis_Schnabel_1979}.
The variational problem defined by~\eqref{eq:intp-var} and~\eqref{eq:df-psb} is known as the \emph{derivative-free symmetric Broyden update} (see~\cite{Powell_2013},~\cite[\S~3.6]{Zhang_2012}, and~\cite[\S~2.4.2]{Ragonneau_2022}).
The two underdetermined interpolation schemes assume that
\begin{equation*}
    n + 1 \le \card(\xpt[k]) \le \frac{1}{2} (n + 1) (n + 2).
\end{equation*}
Otherwise,~$\objm[k]$ could not be uniquely defined (or even not defined at all).
In practice, methods that employ these schemes often enforce~$\card(\xpt[k]) \ge n + 2$ as all the models~$\objm[k]$ would be linear if~$\card(\xpt[k]) = n + 1$.
% Quadratic models of~$\con$ can be built in a similar way.
Finally, if the constraint functions underlying the feasible set~$\fset$ in~\eqref{eq:nlp} are nonlinear, model-based \gls{dfo} methods usually approximates them using similar techniques.

Normally, interpolation-based \gls{dfo} methods do not build the interpolation point~$\xpt[k]$ \emph{ab initio} at each iteration.
Instead, they build an initial interpolation set~$\xpt[0]$ and then  update~$\xpt[k]$ to build~$\xpt[k + 1]$.
For example, only one point differs between~$\xpt[k]$ and~$\xpt[k + 1]$ in Powell's \gls{dfo} methods and \solvername{cobyqa} during usual iterations.
This mechanism prevents the methods from performing unnecessary function evaluations, which are often considered as the principal measure of complexity of \gls{dfo} methods.
However, it is clear that the initial interpolation set~$\xpt[0]$ must be carefully chosen, as it will impact the optimization method for many iterations.
In the unconstrained case, i.e., when~$\fset = \R^n$ in~\eqref{eq:nlp}, Powell proposed an initial interpolation set~$\xpt[0]$ that is employed by many interpolation-based \gls{dfo} methods, including \solvername{uobyqa}, \solvername{newuoa}, \solvername{bobyqa}, \solvername{lincoa}, and \solvername{cobyqa}.
This paper shows that this interpolation set is optimal in a well-poisedness sense detailed in Section~\ref{sec:well-poisedness}.

This paper is organized as follows.\fxnote{Write down the organization of the paper}

\section{Well-poisedness of interpolation sets}
\label{sec:well-poisedness}

In this section, we denote by
\begin{equation*}
    \xpt = \set{y^1, y^2, \dots, y^m} \subseteq \R^n,
\end{equation*}
a set, and we interpolate the objective function~$\obj$ by a quadratic polynomial~$\objm$ on~$\xpt$, i.e.,
\begin{equation}
    \label{eq:intp-cond-gen}
    \objm(y^i) = \obj(y^i), \quad i \in \set{1, 2, \dots, m}.
\end{equation}
More precisely, we let~$\objm$ solve~\eqref{eq:intp-var}, where~$\mathcal{F}^k$ is defined by~\eqref{eq:mnh}.
Note that if~$\xpt$ is seen as an initial interpolation set~$\xpt[0]$, then~$\objm$ denotes the initial model~$\objm[0]$, and the functional~$\mathcal{F}^k = \mathcal{F}^0$ is equivalent to the one defined by~\eqref{eq:df-psb}.
In other words,~$\objm$ is also obtained by the derivative-free symmetric Broyden update.
The following definition guarantees that the interpolation problem is always well-posed.

\begin{definition}[Poisedness]
    The set~$\xpt$ is \emph{poised} in the minimum Frobenius norm sense if the solution to problem~\eqref{eq:intp-var} exists and is unique for any real-valued function~$\obj$.
\end{definition}

As we mentioned earlier, a necessary condition for~$\xpt$ to be poised is
\begin{equation*}
    n + 1 \le m \le \frac{1}{2} (n + 1) (n + 2).
\end{equation*}
Moreover, observe that the KKT system for the variational problem~\eqref{eq:intp-var} is linear with respect to the coefficient of~$\objm$.
Hence, the poisedness of~$\xpt$ is related to the nonsingularity of the coefficient matrix of this KKT system.
Intuitively, the interpolation set~$\xpt$ can be said well-poised if this matrix is well-conditioned.
To formalize this intuition, we first introduce the notion of minimum Frobenius norm Lagrange polynomials.

\subsection{Minimum Frobenius norm Lagrange polynomials}

We assume that the interpolation set~$\xpt$ is poised.
To formally define a measure of well-poisedness of~$\xpt$, we first need to extend the classical definition of the Lagrange polynomials as follows.

\begin{definition}[Minimum Frobenius norm Lagrange polynomials]
    \label{def:min-norm-lagp}
    For each~$i \in \set{1, 2, \dots, m}$, the~$i$th \emph{minimum Frobenius norm Lagrange polynomial}~$\lagp[i]$ for the interpolation problem~\eqref{eq:intp-cond-gen} is the unique quadratic polynomial solving
    \begin{equation*}
        \begin{aligned}
            \min_{Q \in \qpoly} & \quad \norm{\nabla^2 Q}_{\mathsf{F}}\\
            \text{s.t.}         & \quad Q(y^j) = \delta_{i, j}, \quad j \in \set{1, 2, \dots, m},
        \end{aligned}
    \end{equation*}
    where~$\delta_{i, j}$ denotes the Kronecker delta.
\end{definition}

Remark that if~$m = (n + 1) (n + 2) / 2$, then Definition~\ref{def:min-norm-lagp} reduces to the classical definition of the Lagrange polynomials.
Moveover, it can be shown that~$\objm$ is given by
\begin{equation*}
    \objm(x) = \sum_{i = 1}^m \obj(y^i) \lagp[i](x),
\end{equation*}
for any~$x \in \R^n$.

\subsection{Well-poisedness in the minimum Frobenius norm sense}

We are now equiped to define the notion of~$\Lambda$-poisedness in the minimum Frobenius norm sense.

\begin{definition}[$\Lambda$-poisedness in the minimum Frobenius norm sense~{\cite[Definition~5.6]{Conn_Scheinberg_Vicente_2009}}]
    \label{def:lambda-poisedness}
    The poised interpolation set~$\xpt$ is said to be~\emph{$\Lambda$-poised in the minimum Frobenius norm sense} in a compact set~$\mathcal{C} \subseteq \R^n$, for some~$\Lambda > 0$, if
    \begin{equation*}
        \Lambda \ge \max_{1 \le i \le m} \max_{x \in \mathcal{C}} \abs{\lagp[i](x)}.
    \end{equation*}
\end{definition}

If~$\xpt$ is~$\Lambda_0$-poised in the minimum Frobenius norm sense in~$\mathcal{C}$, it is obviously~$\Lambda$-poised in the minimum Frobenius norm sense in~$\mathcal{C}$ for all~$\Lambda \ge \Lambda_0$.
Moreover, one can show that~$\xpt$ is~$\Lambda$-poised if and only if the condition number of the coefficient matrix of the KKT system of~\eqref{eq:intp-var} is bounded by some terms proportional to~$\Lambda$~\cite[Theorem~5.8]{Conn_Scheinberg_Vicente_2009}.
Therefore, the notion of~$\Lambda$-poisedness formalizes the intuitive notion of well-poisedness mentioned above.

\section{Optimality of Powell's initial interpolation set}

Recall that Powell proposed an initial interpolation set~$\xpt[0]$ that is employed by many interpolation-based \gls{dfo} methods and in particular trust-region methods, including \solvername{uobyqa}, \solvername{newuoa}, and in the unconstrained case, \solvername{bobyqa}, \solvername{lincoa}, and \solvername{cobyqa}.
This section first presents this interpolation set, and then shows that it is optimal in term of the least constant of~$\Lambda$-poisedness.

\subsection{Description of the initial interpolation set}

The initial interpolation set for unconstrained optimization proposed by Powell is defined as follows.
Without loss of generality, we assume that the calculations are shifted to the origin.
Let~$\Delta > 0$ be fixed and for~$i \in \set{1, 2, \dots, 2n + 1}$, let~$y^i \in \R^n$ be
\begin{equation}
    \label{eq:set-def}
    y^i =
    \begin{cases}
        0,                      & \text{if~$i = 1$,}\\
        \Delta e_{i - 1},       & \text{if~$2 \le i \le n + 1$,}\\
        -\Delta e_{i - n - 1},  & \text{otherwise},
    \end{cases}
\end{equation}
where~$e_i \in \R^n$ denotes the $i$th standard coordinate vector in~$\R^n$.
The initial interpolation sets~$\xpt[0]_m \subseteq \R^n$ for each~$m \in \set{n + 2, n + 3, \dots, 2n + 1}$ are then defined by
\begin{equation*}
    \xpt[0]_m = \set{y^1, y^2, \dots, y^m}.
\end{equation*}
The default value for~$m$ proposed by Powell is~$2n + 1$.
This is a natural choice, as the corresponding interpolation set is geometrically appealing.
It consist indeed of the origin and equidistant points to the origin on the positive and negative coordinate directions.
In what follows, we provide a theory that justifies this natural choice.

\subsection{Optimality of the interpolation set}

All the methods presented above that are using the initial interpolation set~$\xpt[0]_{2n + 1}$ are trust-region methods, and~$\Delta$ denotes the initial trust-region radius.
Therefore, it is legitimate to study the properties of~$\xpt[0]_m$ for different values of~$m$ in the closed~$\ell_p$-norm ball of radius~$\Delta$, namely
\begin{equation*}
    \mathcal{B}_p(\Delta) = \set{x \in \R^n : \norm{x}_p \le \Delta},
\end{equation*}
with~$p \in [1, \infty]$.
Note that we allow~$p = \infty$.
In the sequel, we study the~$\Lambda$-poisedness of the set~$\xpt[0]_m$ in~$\mathcal{B}_p(\Delta)$.
We will show that~$\xpt[0]_{2n + 1}$ is optimal in the sense that it achieves the least possible constant of~$\Lambda$-poisedness in~$\mathcal{B}_p(\Delta)$ for all~$p \in [1, 2]$.

\subsubsection{Formulation of the Lagrange polynomials}

According to Definition~\ref{def:lambda-poisedness}, the set~$\xpt[0]_m$ is~$\Lambda_p$-poised in~$\mathcal{B}_p(\Delta)$ in the minimum Frobenius norm sense with
\begin{equation*}
    \Lambda_p = \max_{1 \le i \le m} \max_{\norm{x}_p \le \Delta} \abs{\lagp[i](x)},
\end{equation*}
where~$\lagp[i]$ is the~$i$th minimum Frobenius norm Lagrange polynomial associated with~$\xpt[0]_m$ for~$i \in \set{1, 2, \dots, m}$.

To study~$\Lambda_p$, we first present explicit formulae for~$\lagp[i]$ for all~$i \in \set{1, 2, \dots, m}$.
These formulae are given in~\cite[\S~3]{Powell_2006}, without a proof.

\begin{lemma}
    \label{lem:lagp}
    For each~$m \in \set{n + 2, n + 3, \dots,  2n + 1}$ and all~$x \in \R^n$, we have
    \begin{equation*}
        \lagp[i](x) =
        \begin{cases}
            \displaystyle 1 - \frac{1}{\Delta^2} \sum_{j = 1}^{m - n - 1} x_j^2 - \frac{1}{\Delta} \sum_{j = m - n}^n x_j,  & \text{if~$i = 1$,}\\[2ex]
            \displaystyle \frac{x_{i - 1}^2}{2\Delta^2} + \frac{x_{i - 1}}{2\Delta},                                        & \text{if~$2 \le i \le m - n$,}\\[2ex]
            \displaystyle \frac{x_{i - 1}}{\Delta},                                                                         & \text{if~$m - n + 1 \le i \le n + 1$,}\\[2ex]
            \displaystyle \frac{x_{i - 1}^2}{2\Delta^2} - \frac{x_{i - 1}}{2\Delta}.                                        & \text{otherwise.}
        \end{cases}
    \end{equation*}
    In the formulation of~$\lagp[1]$, if~$m = 2n + 1$, we define~$\sum_{j = m - n}^n x_j = 0$.
\end{lemma}

\begin{proof}
    Let~$i \in \set{1, 2, \dots, m}$ be fixed and let~$\lagp$ be a quadratic polynomial that satisfies
    \begin{equation}
        \label{eq:lagp-proof}
        \lagp(y^j) = \delta_{i, j}, \quad j \in \set{1, 2, \dots, m}.
    \end{equation}
    First, it is straightforward to verify that~$\lagp[i]$ satisfies the interpolation conditions~\eqref{eq:lagp-proof}.
    Hence, it suffices to show that the Frobenius norm of its Hessian matrix is least.
    According to the equation~\eqref{eq:set-def}, we have~$y^1 = 0$, and for any~$j \in \set{1, 2, \dots, m - n - 1}$, we have~$y^{j + 1} = \Delta e_j$ and~$y^{n + j + 1} = -\Delta e_j$.
    Therefore,
    \begin{equation*}
        \begin{cases}
            \displaystyle \lagp(y^{j + 1}) = \lagp(y^1) + \Delta \nabla \lagp(y^1)^{\T} e_j + \frac{\Delta^2}{2} e_j^{\T} (\nabla^2 \lagp) e_j,\\[2ex]
            \displaystyle \lagp(y^{n + j + 1}) = \lagp(y^1) - \Delta \nabla \lagp(y^1)^{\T} e_j + \frac{\Delta^2}{2} e_j^{\T} (\nabla^2 \lagp) e_j,
        \end{cases}
    \end{equation*}
    and hence,
    \begin{equation*}
        e_j^{\T} (\nabla^2 \lagp) e_j = \frac{\lagp(y^{j + 1}) + \lagp(y^{n + j + 1}) - 2 \lagp(y^1)}{\Delta^2}.
    \end{equation*}
    This fixes the first~$m - n - 1$ diagonal entries of~$\nabla^2 \lagp$, which are exactly those of~$\nabla^2 \lagp[i]$.
    Since all the other entries of~$\nabla^2 \lagp[i]$ are zero, we have
    \begin{equation*}
        \norm[\big]{\nabla^2 \lagp[i]}_{\mathsf{F}}^2 \le \norm[\big]{\nabla^2 \lagp}_{\mathsf{F}}^2,
    \end{equation*}
    which completes the proof.
\end{proof}

\subsubsection{Bounds for the~\texorpdfstring{$\Lambda$}{\textLambda}-poisedness}

The next lemma simplifies the value of~$\Lambda_p$ for further computations.

\begin{lemma}
    \label{lem:lambda-p}
    For any~$m \in \set{n + 2, n + 3, \dots, 2n + 1}$ and any~$p \in [1, \infty]$, we have
    \begin{equation}
        \label{eq:lambda-p}
        \Lambda_p = \max_{\norm{x}_p \le \Delta} \abs{\lagp[1](x)}.
    \end{equation}
\end{lemma}

\begin{proof}
    For each~$i \in \set{2, 3, \dots, n + 1}$, according to Lemma~\ref{lem:lagp},~$\lagp[i](x)$ depends only on~$x_{i - 1}$ for all~$x \in \R^n$, and hence
    \begin{equation*}
        \max_{\norm{x}_p \le \Delta} \abs{\lagp[i](x)} = \max_{t \in [-\Delta, \Delta]} \abs{\lagp[i](t e_{i - 1})} = 1.
    \end{equation*}
    Similarly, for each~$i \in \set{n + 2, n + 3, \dots, m}$, since~$\lagp[i](x)$ depends only on~$x_{i - n - 1}$ for all~$x \in \R^n$, we have
    \begin{equation*}
        \max_{\norm{x}_p \le \Delta} \abs{\lagp[i](x)} = \max_{t \in [-\Delta, \Delta]} \abs{\lagp[i](t e_{i - n - 1})} = 1.
    \end{equation*}
    Noting that~$\lagp[1](y^1) = 1$ and~$y^1 \in \mathcal{B}_p(\Delta)$, this shows that~\eqref{eq:lambda-p} holds.
\end{proof}

We are now equipped to develop bounds on~$\Lambda_p$ in the general case.

\begin{theorem}
    \label{thm:lambda-p-bounds}
    For any~$m \in \set{n + 2, n + 3, \dots, 2n + 1}$ and any~$p \in [1, \infty]$, we have
    \begin{equation*}
        1 + (2n + 1 - m)^{\frac{p - 1}{p}} \le \Lambda_p \le n,
    \end{equation*}
    where we assume that~$0^0 = 0$ and that the lower bound is~$2n + 2 - m$ for~$p = \infty$.
\end{theorem}

\begin{proof}
    We will establish the bounds using the formulation of~$\Lambda_p$ in Lemma~\ref{lem:lambda-p}.
    For the lower bound, by considering only the points in~$\R^n$ whose leading~$m - n - 1$ components are zeros and whose remaining~$2n + 1 - m$ components are equal, we have
    \begin{equation*}
        \Lambda_p \ge \max_{t \in \R} \set{1 - \frac{t}{\Delta} (2n + 1 - m) : (2n + 1 - m) \abs{t}^p \le \Delta^p} = 1 + (2n + 1 - m)^{\frac{p - 1}{p}}.
    \end{equation*}

    We now prove the upper bound.
    Note that for any~$p \ge 1$, we have~$\mathcal{B}_p(\Delta) \subseteq \mathcal{B}_{\infty}(\Delta)$, so that~$\Lambda_p \le \Lambda_{\infty}$.
    Therefore, we only need to show that~$\Lambda_{\infty} \le n$.
    Considering both~$\lagp[1]$ and~$-\lagp[1]$, we obtain
    \begin{equation*}
        \Lambda_{\infty} = \max_{\norm{x}_{\infty} \le \Delta} \abs{\lagp[1](x)} = \max \set{2n + 2 - m, n - 1} \le n.
    \end{equation*}
\end{proof}

\subsubsection{Some special cases}

We now calculate the exact value of~$\Lambda_p$ in some special cases.

\begin{proposition}
    For any~$m \in \set{n + 2, n + 3, \dots, 2n + 1}$, we have
    \begin{equation}
        \label{eq:lambda-p-1}
        \Lambda_1 =
        \begin{cases}
            2,  & \text{if~$n + 2 \le m \le 2n$,}\\
            1,  & \text{otherwise.}
        \end{cases}
    \end{equation}
\end{proposition}

\begin{proof}
    According to Theorem~\ref{thm:lambda-p-bounds},~$\Lambda_1$ is lower bounded by the right-hand side of~\eqref{eq:lambda-p-1}.
    Therefore, we only need to prove that this right-hand side is also a lower bound for~$\Lambda_1$, using the formulation in Lemma~\ref{lem:lambda-p}.

    For any~$x \in \mathcal{B}_1(\Delta)$, we have
    \begin{equation*}
        \lagp[1](x) \le 1 - \frac{1}{\Delta} \sum_{j = m - n}^n x_j \le 1 + \frac{1}{\Delta} \sum_{j = m - n}^n \abs{x_j}.
    \end{equation*}
    Therefore,
    \begin{equation}
        \label{eq:lambda-p-1-proof-1}
        \lagp[1](x) \le
        \begin{cases}
            2,  & \quad \text{if~$n + 2 \le m \le 2n$,}\\
            1,  & \quad \text{otherwise.}
        \end{cases}
    \end{equation}
    On the other hand,
    \begin{equation}
        \label{eq:lambda-p-1-proof-2}
        \lagp[1](x) = 1 - \sum_{j = 1}^{m - n - 1} \frac{x_j^2}{\Delta^2} - \sum_{j = m - n}^n \frac{x_j}{\Delta} \ge 1 - \sum_{j = 1}^{m - n - 1} \frac{\abs{x_j}}{\Delta} - \sum_{j = m - n}^n \frac{\abs{x_j}}{\Delta} \ge 1 - \frac{\norm{x}_1}{\Delta} \ge 0.
    \end{equation}
    We conclude the proof by combining~\eqref{eq:lambda-p-1-proof-1} and~\eqref{eq:lambda-p-1-proof-2} with Lemma~\ref{lem:lambda-p}.
\end{proof}

\begin{proposition}
    For any~$m \in \set{n + 2, n + 3, \dots, 2n + 1}$, we have
    \begin{equation}
        \label{eq:lambda-p-2}
        \Lambda_2 = 1 + \sqrt{2n + 1 - m}.
    \end{equation}
\end{proposition}

\begin{proof}
    If~$m = 2n + 1$, Lemma~\ref{lem:lagp} tells us that~$\lagp[1](x) = 1 - \norm{x}_2^2 / \Delta^2$ for~$x \in \mathcal{B}_2{\Delta}$.
    Therefore, Lemma~\ref{lem:lambda-p} directly provides the desired result~$\Lambda_2 = 1$.
    We now focus on the case with~$n + 2 \le m < 2n + 1$.

    According to Theorem~\ref{thm:lambda-p-bounds},~$\Lambda_2$ is lower bounded by the right-hand side of~\ref{eq:lambda-p-2}.
    Therefore, we only need to prove that this right-hand side is also an upper bound for~$\Lambda_2$, using the formulation in Lemma~\ref{lem:lambda-p}.

    For any~$x \in \mathcal{B}_2{\Delta}$, we have
    \begin{align}
        \lagp[1](x) &= 1 - \frac{1}{\Delta^2} \sum_{j = 1}^{m - n - 1} x_j^2 - \frac{1}{\Delta} \sum_{j = m - n}^n x_j \nonumber\\
                    &\ge 1 - \frac{1}{\Delta^2} \bigg( \Delta^2 - \sum_{j = m - n}^n x_j^2 \bigg) - \frac{1}{\Delta} \sum_{j = m - n}^n x_j = \sum_{j = m - n}^n \frac{x_j}{\Delta} \bigg( 1 - \frac{x_j}{\Delta} \bigg) \nonumber\\
                    &\ge \min_{y \in \mathcal{B}_2(1)} \sum_{j = m - n}^n y_j (1 - y_j). \label{eq:lambda-p-2-proof}
    \end{align}
    Let~$y^{\ast} \in \mathcal{B}_2(1)$ be a minimizer in~\eqref{eq:lambda-p-2-proof}.
    The first-order necessary condition ensures that there exists a Lagrange multiplier~$\lambda^{\ast} \ge 0$ such that~$1 - 2 y_j^{\ast} + 2 \lambda^{\ast} y_j^{\ast} = 0$ for all~$j \in \set{m - n, \dots, n}$.
    Therefore, the last~$(2n + 1 - m)$ components of~$y^{\ast}$ are equal, and hence,
    \begin{align*}
        \lagp[1](x) \ge \min_{\norm{y}_2 \le 1} \sum_{j = m - n}^n y_j (1 - y_j)    & = \min_{t \in \R} \set{(2n + 1 - m) t (1 - t) : (2n + 1 - m) t^2 \le 1}\\
                                                                                    & = -1 - \sqrt{2n + 1 - m}.
    \end{align*}

    Furthermore,
    \begin{equation*}
        \lagp[1](x) \le 1 + \sum_{j = m - n}^n \frac{\abs{x_j}}{\Delta} \le 1 + \sqrt{2n + 1 - m} \sum_{j = m - n}^n \frac{x_j^2}{\Delta^2} \le 1 + \sqrt{2n + 1 - m}.
    \end{equation*}
    Therefore,~$\abs{\lagp[1](x)} \le 1 + \sqrt{2n + 1 - m}$, and hence, according to Lemma~\ref{lem:lambda-p} and Theorem~\ref{thm:lambda-p-bounds}, we have
    \begin{equation*}
        \Lambda_2 = \max_{\norm{x}_2 \le \Delta} \abs{\lagp[1](x)} = 1 + \sqrt{2n + 1 - m},
    \end{equation*}
    which concludes the proof.
\end{proof}

Before developing the last special value of~$\Lambda_p$ we introduce the following lemma.

\begin{lemma}
    \label{lem:max-norm-pq}
    For any~$p \ge 1$ and~$q \ge 1$, we have
    \begin{subequations}
        \begin{numcases}{\max_{\norm{x}_q \le 1} \norm{x}_p =}
            1,                      & \text{if~$p \ge q$,} \label{eq:max-norm-pq-1}\\
            n^{\frac{q - p}{pq}},   & \text{otherwise.} \label{eq:max-norm-pq-2}
        \end{numcases}
    \end{subequations}
\end{lemma}

\begin{proof}
    Let us first consider the case where~$p \ge q$.
    For~$x \in \mathcal{B}_q(1)$, we have~$\norm{x}_p \le 1$, and this bound is attained at the first coordinate vector~$e_1 \in \mathcal{B}_q(1)$, so that~\eqref{eq:max-norm-pq-1} holds.

    We now consider the case where~$p < q$.
    Let~$e \in \R^n$ be the all-one vector,~$r = q/p$, and~$s = r / (r - 1) = q / (q - p)$.
    For~$x \in \mathcal{B}_q(1)$, define~$y = (\abs{x_1}^p, \abs{x_2}^p, \dots, \abs{x_n}^p)$.
    According to the H{\"{o}}lder inequality, we have
    \begin{equation*}
        \norm{x}_p  = (e^{\T} y)^{\frac{1}{p}} \le (\norm{e}_s \norm{y}_r)^{\frac{1}{p}} = n^{\frac{q - p}{pq}} \norm{x}_q \le n^{\frac{q - p}{pq}}.
    \end{equation*}
    Moreover, this bound is attained by~$x = n^{-\frac{1}{q}}e$, which proves~\eqref{eq:max-norm-pq-2}.
\end{proof}

\begin{proposition}
    For any~$p \ge 1$, if~$m = 2n + 1$, then
    \begin{equation*}
        \Lambda_p = \max \set[\big]{1, n^{\frac{p - 2}{p}} - 1}.
    \end{equation*}
\end{proposition}

\begin{proof}
    It is clear that
    \begin{equation}
        \label{eq:lambda-p-infty-proof-1}
        \max_{\norm{x}_p \le \Delta} \lagp[1](x) = \max_{\norm{x}_p \le \Delta} \bigg( 1 - \frac{\norm{x}_2^2}{\delta^2} \bigg) = 1.
    \end{equation}
    Moreover, according to Lemma~\ref{lem:max-norm-pq}, we have
    \begin{equation}
        \label{eq:lambda-p-infty-proof-2}
        \max_{\norm{x}_p \le \Delta} -\lagp[1](x) = \max_{\norm{x}_p \le \Delta} \frac{\norm{x}_2^2}{\delta^2} - 1 =
        \begin{cases}
            0,                          & \quad \text{if~$p \le 2$,}\\
            n^{\frac{p - 2}{p}} - 1,    & \quad \text{otherwise.}
        \end{cases}
    \end{equation}
    The desired result is obtained by combining~\eqref{eq:lambda-p-infty-proof-1} and~\eqref{eq:lambda-p-infty-proof-2} with Lemma~\ref{lem:lambda-p}.
\end{proof}

\section{Conclusion}

\bibliographystyle{amsplain}
\bibliography{\jobname}

\listoffixmes

\end{document}